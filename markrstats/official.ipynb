{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch tweets: 401 - {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "Data collection completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "bearer_token = os.environ.get(\"TWITTER_BEARER_TOKEN\")\n",
    "user_id = \"1498510162004361220\"  # Replace with the actual user ID\n",
    "\n",
    "url = f\"https://api.twitter.com/2/users/{user_id}/tweets\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {bearer_token}\"\n",
    "}\n",
    "\n",
    "start_time = \"2022-03-22T00:00:00Z\"  # Start of the day in UTC\n",
    "end_time = \"2022-03-26T23:59:59Z\"    # End of the day in UTC\n",
    "\n",
    "params = {\n",
    "    \"max_results\": 100,  # Maximum number of tweets per request\n",
    "    \"tweet.fields\": \"created_at\",  # Request the creation date of the tweet\n",
    "    #\"start_time\": start_time,\n",
    "    #\"end_time\": end_time\n",
    "}\n",
    "\n",
    "csv_file = \"tweets.csv\"  # Path to the CSV file\n",
    "file_exists = os.path.isfile(csv_file)  # Check if the CSV file already exists\n",
    "\n",
    "next_token = None\n",
    "delay = 360  # Initial delay in seconds to stay well within the rate limit\n",
    "\n",
    "dfs = []\n",
    "\n",
    "while True:\n",
    "    if next_token:\n",
    "        params['pagination_token'] = next_token  # Set the pagination token\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        tweets_data = response.json()\n",
    "        tweets = tweets_data.get('data', [])\n",
    "        print(tweets_data)\n",
    "\n",
    "        # Convert the current batch of tweets to a DataFrame\n",
    "        df = pd.DataFrame(tweets)\n",
    "        \n",
    "        # Append the DataFrame to the CSV file\n",
    "        df.to_csv(csv_file, mode='a', index=False, header=not file_exists)\n",
    "        dfs.append(df)\n",
    "        file_exists = True  # Ensure header is not written again\n",
    "        \n",
    "        print(f\"Appended {len(tweets)} tweets to list\")\n",
    "\n",
    "        # Check if there's another page of results\n",
    "        next_token = tweets_data.get('meta', {}).get('next_token', None)\n",
    "        if not next_token:\n",
    "            break  # Exit the loop if there are no more tweets to fetch\n",
    "\n",
    "        # Delay before the next request\n",
    "        time.sleep(delay)\n",
    "        \n",
    "    elif response.status_code == 429:\n",
    "        print(\"Rate limit exceeded. Waiting before retrying...\")\n",
    "        # Extract the time to wait from the headers if available\n",
    "        reset_time = int(response.headers.get('x-rate-limit-reset', time.time() + 900)) - int(time.time())\n",
    "        delay = max(reset_time, 2)  # Ensure delay is at least 2 seconds\n",
    "        time.sleep(delay)\n",
    "    else:\n",
    "        print(f\"Failed to fetch tweets: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "print(\"Data collection completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
